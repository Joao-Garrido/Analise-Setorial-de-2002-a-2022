{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "243214a0",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "b2829db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import logging\n",
    "import warnings\n",
    "from datetime import datetime, timedelta\n",
    "from collections import defaultdict\n",
    "from io import StringIO\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c612879",
   "metadata": {},
   "source": [
    "#### Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "0ba3758b",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
    "    datefmt=\"%H:%M:%S\",\n",
    ")\n",
    "log = logging.getLogger(\"ElectoralCycles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b1d3a5",
   "metadata": {},
   "source": [
    "### Global Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "898512c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ARQUIVO_ENTRADA = \"resultados_analise_b3_com_tickers.xlsx\"\n",
    "SHEET_NAME = \"LISTA FINAL (Cont+IPOs-Canc)\"\n",
    "OUTPUT_DIR = \"./output_v3\"\n",
    "\n",
    "# ---- Datas eleitorais (1º e 2º turnos) -----------------------------------\n",
    "DATAS_PRIMEIRO_TURNO = {\n",
    "    2002: pd.Timestamp(\"2002-10-06\"),\n",
    "    2006: pd.Timestamp(\"2006-10-01\"),\n",
    "    2010: pd.Timestamp(\"2010-10-03\"),\n",
    "    2014: pd.Timestamp(\"2014-10-05\"),\n",
    "    2018: pd.Timestamp(\"2018-10-07\"),\n",
    "    2022: pd.Timestamp(\"2022-10-02\"),\n",
    "}\n",
    "DATAS_SEGUNDO_TURNO = {\n",
    "    2002: pd.Timestamp(\"2002-10-27\"),\n",
    "    2006: pd.Timestamp(\"2006-10-29\"),\n",
    "    2010: pd.Timestamp(\"2010-10-31\"),\n",
    "    2014: pd.Timestamp(\"2014-10-26\"),\n",
    "    2018: pd.Timestamp(\"2018-10-28\"),\n",
    "    2022: pd.Timestamp(\"2022-10-30\"),\n",
    "}\n",
    "ANOS_ELEITORAIS = sorted(DATAS_PRIMEIRO_TURNO.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425a8c69",
   "metadata": {},
   "source": [
    "### Setores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "257896e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Setores B3 ----------------------------------------------------------\n",
    "SETORES_B3 = [\n",
    "    \"Bens Industriais\", \"Comunicações\", \"Construção e Transporte\",\n",
    "    \"Consumo Cíclico\", \"Consumo Não Cíclico\", \"Financeiro\",\n",
    "    \"Materiais Básicos\", \"Outros\", \"Petróleo, Gás e Biocombustíveis\",\n",
    "    \"Saúde\", \"Utilidade Pública\",]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802aa1b6",
   "metadata": {},
   "source": [
    "Δ = CAR_estatais − CAR_privadas\n",
    "\n",
    "Se Δ for significativo, isso sugere que o efeito vem especificamente do risco político/regulatório e não de um choque geral que afeta todo mundo igual.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d68ef0",
   "metadata": {},
   "source": [
    "### PARÂMETROS DE JANELAS, FILTRAGEM E ROBUSTEZ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "6c202f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Janela de estimação (MacKinlay, 1997) --------------------------------\n",
    "ESTIMACAO_INICIO_DU = -252    # dias úteis antes do 1º turno\n",
    "ESTIMACAO_FIM_DU = -30        # dias úteis antes do 1º turno\n",
    "\n",
    "# ---- Janelas de evento (dias úteis relativos ao 1º turno) ----------------\n",
    "JANELAS_EVENTO_1T = {\n",
    "    \"antecipacao_45\":  (-45, -1),     # Efeito propaganda / pricing-in\n",
    "    \"antecipacao_60\":  (-60, -1),     # Robustez: janela mais larga\n",
    "    \"reacao_curta_1t\": (-5, +5),      # Reação imediata ao 1º turno\n",
    "    \"reacao_media_1t\": (-10, +10),    # Robustez\n",
    "    \"reacao_ampla_1t\": (-20, +20),    # Robustez: persistência\n",
    "}\n",
    "\n",
    "# ---- Janelas relativas ao 2º turno --------------------------------------\n",
    "JANELAS_EVENTO_2T = {\n",
    "    \"reacao_curta_2t\": (-5, +5),      # Reação imediata ao 2º turno\n",
    "    \"reacao_media_2t\": (-10, +10),    # Robustez: digestão do resultado\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911eaea6",
   "metadata": {},
   "source": [
    "Janela \"entre turnos\" De [-5 d.u. antes do 1ºT] até [-1 d.u. antes do 2ºT]\n",
    "\n",
    "Período de máxima incerteza: mercado sabe os finalistas mas não o resultado.\n",
    "\n",
    "Tamanho variável (~15-18 d.u. por ano). O N de dias é registrado no output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "1c01e9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "JANELA_ENTRE_TURNOS = True    # flag para ativar\n",
    "\n",
    "# ---- Janelas semestrais (ciclo interno) ----------------------------------\n",
    "JANELAS_CICLO = True          # 1º sem (expectativa) vs 2º sem (disputa)\n",
    "\n",
    "# ---- Janela estendida ----------------------------------------------------\n",
    "JANELA_ESTENDIDA = True       # últimos 6 meses do ano eleitoral\n",
    "\n",
    "JANELAS_ESPELHO = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff88fd0a",
   "metadata": {},
   "source": [
    "\n",
    "ANOS-ESPELHO:\n",
    "\n",
    "   Ano eleitoral → Espelho (T-1) → Contexto do espelho\n",
    "\n",
    "   2002         → 2001           → Pós-crise Argentina, 11 de setembro\n",
    "\n",
    "   2006         → 2005           → Mensalão (crise política, não eleitoral)\n",
    "\n",
    "   2010         → 2009           → Recuperação pós-crise 2008\n",
    "\n",
    "   2014         → 2013           → Jornadas de Junho (protestos em massa)\n",
    "\n",
    "   2018         → 2017           → Joesley Day (mai/17), recuperação no 2º sem\n",
    "   \n",
    "   2022         → 2021           → Pós-COVID, boom de IPOs, Selic subindo\n",
    "\n",
    " Nenhum ano-espelho é perfeitamente \"normal\". Os resultados devem ser\n",
    "interpretados com essa ressalva. Reportar N e contexto em tabela separada."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90762f2f",
   "metadata": {},
   "source": [
    "## Parâmetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "de135188",
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_PREGOES_PCT = 0.80        # Mínimo 80% de pregões no ano para inclusão\n",
    "MIN_EMPRESAS_SETOR = 5        # Corte: mínimo N empresas por setor/ano\n",
    "ANOS_CRISE = [2008, 2014, 2020]    \n",
    "N_PLACEBO_EVENTS = 1000             "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6badd8b0",
   "metadata": {},
   "source": [
    "## Selic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "594e7d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "SELIC_ANUAL = {\n",
    "    2002: 0.1911, 2006: 0.1513, 2010: 0.0975,\n",
    "    2014: 0.1115, 2018: 0.0640, 2022: 0.1275,\n",
    "}\n",
    "IPCA_ANUAL = {\n",
    "    2002: 0.1253, 2006: 0.0314, 2010: 0.0591,\n",
    "    2014: 0.0641, 2018: 0.0375, 2022: 0.0562,\n",
    "}\n",
    "\n",
    "# Selic diária (proxy CDI) para cálculo de Sharpe\n",
    "SELIC_DIARIA = {k: (1 + v) ** (1/252) - 1 for k, v in SELIC_ANUAL.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f308abf1",
   "metadata": {},
   "source": [
    "\n",
    " NEFIN (Núcleo de Economia Financeira da USP) calcula diariamente os\n",
    "fatores de risco Fama-French adaptados para o Brasil:\n",
    "\n",
    "- Mkt-Rf : prêmio de risco de mercado (retorno mercado − CDI)\n",
    "\n",
    "- SMB    : prêmio de tamanho (small caps − large caps)\n",
    "\n",
    "- HML    : prêmio de valor (alto B/M − baixo B/M)\n",
    "\n",
    "- WML    : prêmio de momentum (vencedoras − perdedoras)\n",
    "\n",
    "- IML    : prêmio de iliquidez (ilíquidas − líquidas)   - Rf     : taxa livre de risco diária (CDI)\n",
    "\n",
    "\n",
    "samos para rodar um modelo alternativo de robustez (Fama-French 3):\n",
    "\n",
    "R_i − Rf = α + β·MktRf + s·SMB + h·HML + ε\n",
    "\n",
    "Se o CAR permanece significativo após controlar por tamanho e valor, o resultado é mais robusto contra a crítica de que o efeito capturado\n",
    "\n",
    "seria apenas exposição a small caps ou empresas de valor.\n",
    "\n",
    "\n",
    "Se o download falhar (site fora, sem internet), o script continua normalmente apenas com o Modelo de Mercado (CAPM)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "5d8ddb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Corrigir alguns apenas\n",
    "\n",
    "TICKER_MAPPING = {\n",
    "    \"VVAR3\": \"BHIA3\", \"BTOW3\": \"AMER3\", \"LAME4\": \"LAME3\", \"PCAR4\": \"PCAR3\",\n",
    "    \"KROT3\": \"COGN3\", \"ESTC3\": \"YDUQ3\", \"RAIL3\": \"RUMO3\",\n",
    "    \"BVMF3\": \"B3SA3\", \"CTIP3\": \"B3SA3\",\n",
    "    \"BRIN3\": \"BRML3\", \"BRML3\": \"ALOS3\", \"SMLE3\": \"SMFT3\",\n",
    "    \"LINX3\": \"STNE3\", \"VIVT4\": \"VIVT3\", \"TIMP3\": \"TIMS3\",\n",
    "    \"QGEP3\": \"BRAV3\", \"GNDI3\": \"HAPV3\", \"FIBR3\": \"SUZB3\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f1e981",
   "metadata": {},
   "source": [
    "## ETAPA 1 — Ingestão de Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "7172f1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def carregar_lista_empresas(caminho: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Carrega lista de empresas, aplica mapeamento De-Para,\n",
    "    e lê coluna ESTATAL direto da planilha.\n",
    "    \"\"\"\n",
    "    log.info(\"=\" * 70)\n",
    "    log.info(\"ETAPA 1: INGESTÃO DE DADOS\")\n",
    "    log.info(\"=\" * 70)\n",
    "\n",
    "    df = pd.read_excel(caminho, sheet_name=\"Sheet1\")\n",
    "    df = df.dropna(subset=[\"TICKER\", \"SETOR_B3\"])\n",
    "\n",
    "    # Parse de datas\n",
    "    df[\"DT_REG\"] = pd.to_datetime(df[\"DT_REG\"], errors=\"coerce\")\n",
    "\n",
    "    # Coluna ESTATAL: converte \"Sim\"/\"Não\" para booleano\n",
    "    df[\"ESTATAL\"] = df[\"ESTATAL\"].str.strip().str.upper().eq(\"SIM\")\n",
    "\n",
    "    # Ticker original → mapeado → yfinance\n",
    "    df[\"TICKER_ORIGINAL\"] = df[\"TICKER\"].str.strip()\n",
    "    df[\"TICKER_MAPEADO\"] = df[\"TICKER_ORIGINAL\"].map(TICKER_MAPPING).fillna(\n",
    "        df[\"TICKER_ORIGINAL\"]\n",
    "    )\n",
    "    df[\"TICKER_YF\"] = df[\"TICKER_MAPEADO\"] + \".SA\"\n",
    "\n",
    "    n_map = (df[\"TICKER_ORIGINAL\"] != df[\"TICKER_MAPEADO\"]).sum()\n",
    "    n_est = df[\"ESTATAL\"].sum()\n",
    "\n",
    "    log.info(\"  → %d empresas | %d setores | %d remapeados | %d estatais\",\n",
    "             len(df), df[\"SETOR_B3\"].nunique(), n_map, n_est)\n",
    "\n",
    "    # Log das estatais para conferência\n",
    "    for _, r in df[df[\"ESTATAL\"]].iterrows():\n",
    "        log.info(\"    [ESTATAL] %s — %s (%s)\",\n",
    "                 r[\"TICKER_ORIGINAL\"], str(r[\"DENOM_SOCIAL\"])[:45], r[\"SETOR_B3\"])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "de20b50f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVISO: Usando key hardcoded – configure como env var para segurança!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from alpha_vantage.timeseries import TimeSeries\n",
    "\n",
    "# Forma segura: defina como variável de ambiente (recomendado)\n",
    "# No terminal: export ALPHA_VANTAGE_KEY=LB3KHS0AN1R2E36B (Linux/Mac) ou set no Windows\n",
    "ALPHA_VANTAGE_KEY = os.getenv('ALPHA_VANTAGE_KEY')\n",
    "\n",
    "# Fallback temporário só para teste local (remova em produção!)\n",
    "if not ALPHA_VANTAGE_KEY:\n",
    "    ALPHA_VANTAGE_KEY = 'SSU8PQU94ONCSLAF'  # Apague isso depois de configurar env\n",
    "    print(\"AVISO: Usando key hardcoded – configure como env var para segurança!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "720c0418",
   "metadata": {},
   "outputs": [],
   "source": [
    "def baixar_precos_yfinance(tickers: list, start=\"2001-01-01\", end=\"2023-12-31\"):\n",
    "    \"\"\"Baixa preços ajustados e volumes via yfinance (primário) + Alpha Vantage (fallback).\"\"\"\n",
    "    import yfinance as yf\n",
    "\n",
    "    log.info(\"Baixando preços de %d tickers (primário: yfinance, fallback: Alpha Vantage)...\", len(tickers))\n",
    "    all_close = {}\n",
    "    all_volume = {}\n",
    "    diagnostico_list = []\n",
    "\n",
    "    blocos = [tickers[i:i+50] for i in range(0, len(tickers), 50)]\n",
    "\n",
    "    # Cliente Alpha Vantage (inicializa só se key válida)\n",
    "    ts_av = None\n",
    "    if ALPHA_VANTAGE_KEY:\n",
    "        try:\n",
    "            ts_av = TimeSeries(key=ALPHA_VANTAGE_KEY, output_format='pandas')\n",
    "        except Exception as e:\n",
    "            log.warning(\"Falha ao inicializar Alpha Vantage: %s\", e)\n",
    "\n",
    "    for idx, bloco in enumerate(blocos):\n",
    "        log.info(\"  Bloco %d/%d (%d tickers)\", idx+1, len(blocos), len(bloco))\n",
    "        try:\n",
    "            data = yf.download(bloco, start=start, end=end, auto_adjust=True,\n",
    "                               progress=False, threads=True)\n",
    "            if data.empty:\n",
    "                raise ValueError(\"Dados vazios no yfinance\")\n",
    "\n",
    "            if isinstance(data.columns, pd.MultiIndex):\n",
    "                close = data[\"Close\"]\n",
    "                volume = data[\"Volume\"]\n",
    "            else:\n",
    "                close = data[[\"Close\"]]; close.columns = bloco\n",
    "                volume = data[[\"Volume\"]]; volume.columns = bloco\n",
    "\n",
    "            for col in close.columns:\n",
    "                if close[col].notna().sum() > 0:\n",
    "                    all_close[col] = close[col]\n",
    "                    all_volume[col] = volume[col] if col in volume.columns else pd.Series(dtype=float)\n",
    "                    diagnostico_list.append({\n",
    "                        \"ticker_yf\": col, \"status\": \"ok\", \"fonte\": \"yfinance\", \"motivo\": \"\"\n",
    "                    })\n",
    "                else:\n",
    "                    diagnostico_list.append({\n",
    "                        \"ticker_yf\": col, \"status\": \"falha\", \"fonte\": \"\", \"motivo\": \"no_data_yf\"\n",
    "                    })\n",
    "\n",
    "            for t in bloco:\n",
    "                if t not in close.columns:\n",
    "                    diagnostico_list.append({\n",
    "                        \"ticker_yf\": t, \"status\": \"falha\", \"fonte\": \"\", \"motivo\": \"not_found_yf\"\n",
    "                    })\n",
    "\n",
    "        except Exception as e:\n",
    "            log.warning(\"  Erro yfinance bloco %d: %s → Marcando para fallback\", idx+1, e)\n",
    "            for t in bloco:\n",
    "                diagnostico_list.append({\n",
    "                    \"ticker_yf\": t, \"status\": \"falha\", \"fonte\": \"\", \"motivo\": f\"erro_yf: {str(e)}\"\n",
    "                })\n",
    "\n",
    "        time.sleep(0.3)\n",
    "\n",
    "    # Fallback Alpha Vantage para falhas\n",
    "    falhas = [d for d in diagnostico_list if d[\"status\"] == \"falha\"]\n",
    "    if falhas and ts_av:\n",
    "        log.info(\"  Fallback Alpha Vantage para %d tickers falhos...\", len(falhas))\n",
    "        for diag in falhas:\n",
    "            t = diag[\"ticker_yf\"]\n",
    "            try:\n",
    "                data_av, _ = ts_av.get_daily(symbol=t, outputsize='full')\n",
    "                data_av = data_av.rename(columns={\n",
    "                    '1. open': 'Open', '2. high': 'High', '3. low': 'Low',\n",
    "                    '4. close': 'Close', '5. volume': 'Volume'\n",
    "                })\n",
    "                if data_av['Close'].notna().sum() > 0:\n",
    "                    all_close[t] = data_av['Close']\n",
    "                    all_volume[t] = data_av['Volume']\n",
    "                    diag[\"status\"] = \"ok\"\n",
    "                    diag[\"fonte\"] = \"alpha_vantage\"\n",
    "                    diag[\"motivo\"] = \"\"\n",
    "                \n",
    "                time.sleep(12)  # Rate limit free (5 calls/min)\n",
    "\n",
    "            except Exception as av_e:\n",
    "                log.warning(\"  Falha Alpha Vantage %s: %s\", t, av_e)\n",
    "                diag[\"motivo\"] += f\"; erro_av: {str(av_e)}\"\n",
    "\n",
    "    # Finaliza\n",
    "    df_precos = pd.DataFrame(all_close)\n",
    "    df_precos.index = pd.to_datetime(df_precos.index)\n",
    "    df_volumes = pd.DataFrame(all_volume)\n",
    "    df_volumes.index = pd.to_datetime(df_volumes.index)\n",
    "\n",
    "    df_diag = pd.DataFrame(diagnostico_list)\n",
    "    n_ok = (df_diag[\"status\"] == \"ok\").sum()\n",
    "    n_falha = (df_diag[\"status\"] == \"falha\").sum()\n",
    "\n",
    "    log.info(\"  → OK: %d | Falha: %d (%.1f%%)\", n_ok, n_falha,\n",
    "             100 * n_falha / max(len(tickers), 1))\n",
    "\n",
    "    return df_precos, df_volumes, df_diag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "13fd900b",
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "1f9549d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def baixar_ibovespa(start=\"2000-01-02\", end=\"2023-12-31\"):\n",
    "    import yfinance as yf\n",
    "    log.info(\"Baixando Ibovespa ...\")\n",
    "    ibov = yf.download(\"^BVSP\", start=start, end=end, auto_adjust=True, progress=False)\n",
    "    serie = ibov[\"Close\"].squeeze()\n",
    "    serie.index = pd.to_datetime(serie.index)\n",
    "    serie.name = \"IBOV\"\n",
    "    log.info(\"  → %d obs\", len(serie))\n",
    "    return serie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "7072853b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def baixar_fatores_nefin():\n",
    "    \"\"\"\n",
    "    Baixa fatores Fama-French brasileiros do NEFIN-USP (Versão CSV único).\n",
    "    Retorna DataFrame com colunas: Mkt_Rf, SMB, HML, Rf.\n",
    "    Se falhar, retorna None (script continua sem FF3).\n",
    "    \"\"\"\n",
    "    log.info(\"Baixando fatores Fama-French do NEFIN-USP ...\")\n",
    "    \n",
    "    # URL definida globalmente ou localmente\n",
    "    url_csv = \"https://nefin.com.br/resources/risk_factors/nefin_factors.csv\"\n",
    "    \n",
    "    try:\n",
    "        # Tenta ler o CSV direto da URL\n",
    "        df = pd.read_csv(url_csv)\n",
    "\n",
    "        # 1. Tratamento de Data (O CSV novo tem a coluna 'Date' pronta)\n",
    "        if \"Date\" in df.columns:\n",
    "            df[\"date\"] = pd.to_datetime(df[\"Date\"])\n",
    "            df = df.set_index(\"date\")\n",
    "        else:\n",
    "            # Fallback caso o formato mude para year/month/day\n",
    "            if {\"year\", \"month\", \"day\"}.issubset(df.columns):\n",
    "                 df[\"date\"] = pd.to_datetime(df[[\"year\", \"month\", \"day\"]])\n",
    "                 df = df.set_index(\"date\")\n",
    "            else:\n",
    "                log.warning(\"  Formato de data desconhecido no CSV do NEFIN.\")\n",
    "                return None\n",
    "\n",
    "        # 2. Renomear colunas para o padrão do script (Mkt_Rf, SMB, HML, Rf)\n",
    "        # O CSV vem como: Rm_minus_Rf, SMB, HML, Risk_Free\n",
    "        rename_map = {\n",
    "            \"Rm_minus_Rf\": \"Mkt_Rf\", \n",
    "            \"Risk_Free\": \"Rf\"\n",
    "        }\n",
    "        df = df.rename(columns=rename_map)\n",
    "\n",
    "        # 3. Filtrar apenas as colunas necessárias\n",
    "        cols_necessarias = [\"Mkt_Rf\", \"SMB\", \"HML\", \"Rf\"]\n",
    "        \n",
    "        # Verifica se todas existem\n",
    "        if not set(cols_necessarias).issubset(df.columns):\n",
    "            log.warning(f\"  Colunas faltando no NEFIN. Encontradas: {df.columns.tolist()}\")\n",
    "            return None\n",
    "\n",
    "        df_factors = df[cols_necessarias]\n",
    "\n",
    "        log.info(\"  → Fatores NEFIN: %d obs (de %s a %s)\",\n",
    "                 len(df_factors), \n",
    "                 df_factors.index.min().strftime('%Y-%m'), \n",
    "                 df_factors.index.max().strftime('%Y-%m'))\n",
    "        \n",
    "        return df_factors\n",
    "\n",
    "    except Exception as e:\n",
    "        log.warning(\"  Falha ao baixar NEFIN: %s. FF3 desabilitado.\", e)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "93997b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_teste = baixar_fatores_nefin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "65296d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aplicar_filtro_existencia(df_precos, df_empresas):\n",
    "    \"\"\"\n",
    "    Invalida (NaN) precos anteriores a data de registro (DT_REG).\n",
    "    Assume que DT_REG existe e que nao ha cancelamentos.\n",
    "    \"\"\"\n",
    "    log.info(\"Aplicando filtro de existencia (apenas DT_REG)...\")\n",
    "\n",
    "    # Cria copia para nao alterar o original\n",
    "    df = df_precos.copy()\n",
    "\n",
    "    # Remove fuso horario para evitar erro de comparacao com o Excel\n",
    "    if df.index.tz is not None:\n",
    "        df.index = df.index.tz_localize(None)\n",
    "\n",
    "    # Dicionario: Ticker -> Data Inicio\n",
    "    lookup = {}\n",
    "    col_inicio = \"DT_REG\"\n",
    "\n",
    "    for _, row in df_empresas.iterrows():\n",
    "        tk = row[\"TICKER_YF\"]\n",
    "        # Converte direto\n",
    "        reg = pd.to_datetime(row[col_inicio], errors='coerce')\n",
    "\n",
    "        # Se houver duplicata de ticker, preserva a data mais antiga (conservador)\n",
    "        if tk in lookup:\n",
    "            old_reg = lookup[tk]\n",
    "            if pd.notna(reg) and (pd.isna(old_reg) or reg < old_reg):\n",
    "                lookup[tk] = reg\n",
    "        else:\n",
    "            lookup[tk] = reg\n",
    "\n",
    "    total_cortes = 0\n",
    "\n",
    "    # Aplica o filtro coluna por coluna\n",
    "    for col in df.columns:\n",
    "        if col in lookup:\n",
    "            dt_inicio = lookup[col]\n",
    "\n",
    "            if pd.notna(dt_inicio):\n",
    "                # Corta tudo que vier antes da data de registro\n",
    "                mask = df.index < dt_inicio\n",
    "                if mask.any():\n",
    "                    qtd = df.loc[mask, col].notna().sum()\n",
    "                    if qtd > 0:\n",
    "                        total_cortes += qtd\n",
    "                        df.loc[mask, col] = np.nan\n",
    "\n",
    "    log.info(\"  -> Filtro aplicado. Observacoes removidas (pre-inicio): %d\", total_cortes)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9687612",
   "metadata": {},
   "source": [
    "## ETAPA 2 – ÍNDICES SETORIAIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "1b001be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ETAPA 2 – ÍNDICES SETORIAIS (EW + VW) + FILTRO DE LIQUIDEZ\n",
    "# =============================================================================\n",
    "#\n",
    "# Fluxo:\n",
    "#   1. Calcula retornos log de cada ativo individual\n",
    "#   2. Aplica filtro de liquidez por (ticker, ano): exige ≥80% de pregões\n",
    "#   3. Constrói índices setoriais em dois esquemas:\n",
    "#      - Equal-Weighted (EW): média simples dos retornos\n",
    "#      - Volume-Weighted (VW): ponderado por volume financeiro médio 20d\n",
    "#   4. Registra composição (N real de empresas por setor/ano após filtro)\n",
    "#\n",
    "# NOTA sobre retornos log:\n",
    "#   Usamos r_t = ln(P_t / P_{t-1}). A média EW de retornos log não é\n",
    "#   exatamente o retorno de um portfólio equal-weighted (que seria média\n",
    "#   de retornos aritméticos). Para janelas curtas ([-5,+5]) a diferença\n",
    "#   é desprezível. Para janelas de 6 meses, pode importar marginalmente.\n",
    "#   A maioria dos event studies (MacKinlay 1997, Silva et al. 2015) usa\n",
    "#   retornos log. Documentamos aqui por transparência.\n",
    "# ============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "bad9a61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aplicar_filtro_liquidez(df_ret, ano, min_pct=MIN_PREGOES_PCT):\n",
    "    \"\"\"\n",
    "    Filtra tickers que negociaram em menos de min_pct dos pregões no ano.\n",
    "    \n",
    "    Parâmetros:\n",
    "        df_ret    : DataFrame de retornos (index=datas, columns=tickers)\n",
    "        ano       : int, ano para filtrar\n",
    "        min_pct   : float, fração mínima de pregões (default 0.80)\n",
    "    \n",
    "    Retorna:\n",
    "        lista de tickers que passaram no filtro para aquele ano\n",
    "    \"\"\"\n",
    "    mask_ano = df_ret.index.year == ano\n",
    "    ret_ano = df_ret.loc[mask_ano]\n",
    "    \n",
    "    if ret_ano.empty:\n",
    "        return []\n",
    "    \n",
    "    n_pregoes = mask_ano.sum()  # total de pregões no ano\n",
    "    min_obs = int(n_pregoes * min_pct)\n",
    "    \n",
    "    # Conta pregões com dado válido (não-NaN) por ticker\n",
    "    obs_por_ticker = ret_ano.notna().sum()\n",
    "    \n",
    "    aprovados = obs_por_ticker[obs_por_ticker >= min_obs].index.tolist()\n",
    "    return aprovados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "9f71519a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construir_indices_setoriais(df_precos, df_volumes, df_empresas):\n",
    "    \"\"\"\n",
    "    Constrói índices setoriais EW e VW com filtro de liquidez.\n",
    "    \n",
    "    Etapas internas:\n",
    "      1. Retornos log de todos os ativos\n",
    "      2. Para cada (setor, ano): aplica filtro de liquidez\n",
    "      3. EW = média simples dos retornos dos tickers líquidos\n",
    "      4. VW = média ponderada por volume financeiro (proxy market cap)\n",
    "      5. Registra composição real (N após filtro)\n",
    "    \n",
    "    Retorna:\n",
    "        (df_ret_ew, df_ret_vw, df_composicao)\n",
    "    \"\"\"\n",
    "    log.info(\"=\" * 70)\n",
    "    log.info(\"ETAPA 2: ÍNDICES SETORIAIS (EW + VW) COM FILTRO DE LIQUIDEZ\")\n",
    "    log.info(\"=\" * 70)\n",
    "\n",
    "    # Retornos log\n",
    "    ret = np.log(df_precos / df_precos.shift(1))\n",
    "    \n",
    "    # Volume financeiro = preço × volume de ações\n",
    "    vol_fin = df_precos * df_volumes\n",
    "\n",
    "    # Mapeamento ticker → setor\n",
    "    t2s = {}\n",
    "    for _, row in df_empresas.iterrows():\n",
    "        t2s[row[\"TICKER_YF\"]] = row[\"SETOR_B3\"]\n",
    "\n",
    "    # Listas para acumular resultados\n",
    "    series_ew = {}\n",
    "    series_vw = {}\n",
    "    composicao = []\n",
    "\n",
    "    for setor in SETORES_B3:\n",
    "        # Todos os tickers do setor que existem no DataFrame\n",
    "        cols_setor = [c for c in ret.columns if t2s.get(c) == setor]\n",
    "        if not cols_setor:\n",
    "            log.warning(\"  Sem tickers para '%s'\", setor)\n",
    "            continue\n",
    "\n",
    "        ew_parts = []  # pedaços anuais do índice EW\n",
    "        vw_parts = []  # pedaços anuais do índice VW\n",
    "\n",
    "        for ano in range(2001, 2024):\n",
    "            mask_ano = ret.index.year == ano\n",
    "            if mask_ano.sum() == 0:\n",
    "                continue\n",
    "\n",
    "            # Filtro de liquidez: quais tickers do setor têm ≥80% pregões neste ano\n",
    "            aprovados_global = aplicar_filtro_liquidez(ret, ano)\n",
    "            cols_ano = [c for c in cols_setor if c in aprovados_global]\n",
    "            \n",
    "            n_antes = len(cols_setor)\n",
    "            n_depois = len(cols_ano)\n",
    "\n",
    "            composicao.append({\n",
    "                \"setor\": setor,\n",
    "                \"ano\": ano,\n",
    "                \"n_tickers_setor\": n_antes,\n",
    "                \"n_com_dados_liquidos\": n_depois,\n",
    "                \"filtro_removeu\": n_antes - n_depois,\n",
    "            })\n",
    "\n",
    "            if n_depois < MIN_EMPRESAS_SETOR:\n",
    "                # Poucos tickers → NaN para este setor/ano\n",
    "                continue\n",
    "\n",
    "            ret_ano = ret.loc[mask_ano, cols_ano]\n",
    "            \n",
    "            # EW: média simples\n",
    "            ew_parts.append(ret_ano.mean(axis=1))\n",
    "\n",
    "            # VW: ponderado por volume financeiro médio 20d\n",
    "            vf_ano = vol_fin.loc[mask_ano, cols_ano].rolling(20, min_periods=5).mean()\n",
    "            vf_sum = vf_ano.sum(axis=1)\n",
    "            # Evita divisão por zero\n",
    "            weights = vf_ano.div(vf_sum.replace(0, np.nan), axis=0)\n",
    "            vw_parts.append((ret_ano * weights).sum(axis=1))\n",
    "\n",
    "        # Concatena os pedaços anuais\n",
    "        if ew_parts:\n",
    "            series_ew[setor] = pd.concat(ew_parts).sort_index()\n",
    "        if vw_parts:\n",
    "            series_vw[setor] = pd.concat(vw_parts).sort_index()\n",
    "\n",
    "        # Log resumo\n",
    "        comp_setor = [c for c in composicao if c[\"setor\"] == setor]\n",
    "        anos_validos = sum(1 for c in comp_setor if c[\"n_com_dados_liquidos\"] >= MIN_EMPRESAS_SETOR)\n",
    "        log.info(\"  %s: %d tickers totais | %d/%d anos com ≥%d empresas líquidas\",\n",
    "                 setor, len(cols_setor), anos_validos, len(comp_setor), MIN_EMPRESAS_SETOR)\n",
    "\n",
    "    df_ret_ew = pd.DataFrame(series_ew)\n",
    "    df_ret_vw = pd.DataFrame(series_vw)\n",
    "    df_comp = pd.DataFrame(composicao)\n",
    "\n",
    "    log.info(\"  → EW: %d setores × %d datas\", df_ret_ew.shape[1], len(df_ret_ew))\n",
    "    log.info(\"  → VW: %d setores × %d datas\", df_ret_vw.shape[1], len(df_ret_vw))\n",
    "\n",
    "    return df_ret_ew, df_ret_vw, df_comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "02d47b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# TABELA DE SOBREVIVÊNCIA (transparência do viés)\n",
    "# =============================================================================\n",
    "# Compara N empresas que *deveriam* existir (registradas até aquele ano)\n",
    "# vs. quantas *de fato* têm dados de preço.\n",
    "#\n",
    "# NOTA: DT_CANCEL não está na planilha, então n_esperado conta apenas\n",
    "# empresas com DT_REG <= ano. Isso superestima o esperado (inclui\n",
    "# empresas já canceladas), mas é conservador: se mesmo com essa\n",
    "# superestimação a cobertura for alta, o viés é menor.\n",
    "# ============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "82d2c81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gerar_tabela_sobrevivencia(df_empresas, df_precos):\n",
    "    \"\"\"\n",
    "    Tabela de cobertura: N esperado vs. N com dados por setor/ano.\n",
    "    Sem DT_CANCEL, n_esperado = empresas com DT_REG <= ano.\n",
    "    \"\"\"\n",
    "    log.info(\"Gerando tabela de sobrevivência ...\")\n",
    "    \n",
    "    t2s = {row[\"TICKER_YF\"]: row[\"SETOR_B3\"] for _, row in df_empresas.iterrows()}\n",
    "    registros = []\n",
    "\n",
    "    for ano in range(2001, 2024):\n",
    "        mask_ano = df_precos.index.year == ano\n",
    "        if mask_ano.sum() == 0:\n",
    "            continue\n",
    "        \n",
    "        precos_ano = df_precos.loc[mask_ano]\n",
    "\n",
    "        for setor in SETORES_B3:\n",
    "            cols = [c for c in precos_ano.columns if t2s.get(c) == setor]\n",
    "            \n",
    "            # N com dados: tickers que têm ≥1 preço válido no ano\n",
    "            n_dados = precos_ano[cols].notna().any().sum() if cols else 0\n",
    "\n",
    "            # N esperado: empresas registradas até esse ano (sem DT_CANCEL)\n",
    "            n_esp = (\n",
    "                (df_empresas[\"SETOR_B3\"] == setor) &\n",
    "                (df_empresas[\"DT_REG\"].dt.year <= ano)\n",
    "            ).sum()\n",
    "\n",
    "            cobertura = round(100 * n_dados / max(n_esp, 1), 1)\n",
    "\n",
    "            registros.append({\n",
    "                \"ano\": ano,\n",
    "                \"setor\": setor,\n",
    "                \"n_esperado\": n_esp,\n",
    "                \"n_com_dados\": n_dados,\n",
    "                \"cobertura_pct\": cobertura,\n",
    "            })\n",
    "\n",
    "    df_sobrev = pd.DataFrame(registros)\n",
    "    \n",
    "    # Log resumo por ano eleitoral\n",
    "    for ano in ANOS_ELEITORAIS:\n",
    "        sub = df_sobrev[df_sobrev[\"ano\"] == ano]\n",
    "        if sub.empty:\n",
    "            continue\n",
    "        cob_media = sub[\"cobertura_pct\"].mean()\n",
    "        log.info(\"  %d: cobertura média %.1f%% (N esperado=%d, N com dados=%d)\",\n",
    "                 ano, cob_media, sub[\"n_esperado\"].sum(), sub[\"n_com_dados\"].sum())\n",
    "\n",
    "    return df_sobrev"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f86008",
   "metadata": {},
   "source": [
    "## Janelas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "d389e628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# ETAPA 3 – JANELAS DE EVENTO (VERSÃO FINAL SIMPLES E SEGURA)\n",
    "# ============================================================================\n",
    "\n",
    "def offset_du(bdates, data_ref, offset=0):\n",
    "    \"\"\"Retorna a data útil mais próxima com offset.\"\"\"\n",
    "    bdays = bdates.sort_values()\n",
    "    pos = min(bdays.searchsorted(data_ref), len(bdays) - 1)\n",
    "    target = max(0, min(pos + offset, len(bdays) - 1))\n",
    "    return bdays[target]\n",
    "\n",
    "\n",
    "def coletar_todas_janelas(bdates, ano):\n",
    "    \"\"\"\n",
    "    Coleta todas as janelas ativas para um ano eleitoral.\n",
    "    Versão final: força o fim das janelas semestrais/estendida em dezembro do ano correto.\n",
    "    \"\"\"\n",
    "    dt1 = DATAS_PRIMEIRO_TURNO[ano]\n",
    "    dt2 = DATAS_SEGUNDO_TURNO.get(ano)\n",
    "\n",
    "    jans = {}\n",
    "\n",
    "    # 1. Janelas do 1º Turno\n",
    "    for nome, (di, df_) in JANELAS_EVENTO_1T.items():\n",
    "        jans[nome] = (\n",
    "            offset_du(bdates, dt1, di),\n",
    "            offset_du(bdates, dt1, df_)\n",
    "        )\n",
    "\n",
    "    # 2. Janelas do 2º Turno\n",
    "    if dt2:\n",
    "        for nome, (di, df_) in JANELAS_EVENTO_2T.items():\n",
    "            jans[nome] = (\n",
    "                offset_du(bdates, dt2, di),\n",
    "                offset_du(bdates, dt2, df_)\n",
    "            )\n",
    "\n",
    "    # 3. Janela Entre Turnos\n",
    "    if JANELA_ENTRE_TURNOS and dt2:\n",
    "        jans[\"entre_turnos\"] = (\n",
    "            offset_du(bdates, dt1, -5),\n",
    "            offset_du(bdates, dt2, -1)\n",
    "        )\n",
    "\n",
    "    # 4. Janelas Semestrais (corrigido com força)\n",
    "    if JANELAS_CICLO:\n",
    "        jans[\"ciclo_1sem\"] = (\n",
    "            offset_du(bdates, pd.Timestamp(f\"{ano}-01-01\"), 0),\n",
    "            offset_du(bdates, pd.Timestamp(f\"{ano}-06-30\"), 0)\n",
    "        )\n",
    "        jans[\"ciclo_2sem\"] = (\n",
    "            offset_du(bdates, pd.Timestamp(f\"{ano}-07-01\"), 0),\n",
    "            offset_du(bdates, pd.Timestamp(f\"{ano+1}-01-01\"), -1)   # Força último dia útil de dezembro\n",
    "        )\n",
    "\n",
    "    # 5. Janela Estendida (corrigido)\n",
    "    if JANELA_ESTENDIDA:\n",
    "        jans[\"estendida\"] = (\n",
    "            offset_du(bdates, pd.Timestamp(f\"{ano}-07-01\"), 0),\n",
    "            offset_du(bdates, pd.Timestamp(f\"{ano+1}-01-01\"), -1)   # Força fim em dezembro\n",
    "        )\n",
    "\n",
    "    # 6. Janelas-Espelho (T-1)\n",
    "    if JANELAS_ESPELHO:\n",
    "        principais = {k: v for k, v in jans.items() if not k.startswith(\"espelho_\")}\n",
    "        for nome, (ini, fim) in principais.items():\n",
    "            try:\n",
    "                ini_esp = ini - pd.DateOffset(years=1)\n",
    "                fim_esp = fim - pd.DateOffset(years=1)\n",
    "                jans[f\"espelho_{nome}\"] = (\n",
    "                    offset_du(bdates, ini_esp, 0),\n",
    "                    offset_du(bdates, fim_esp, 0)\n",
    "                )\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    return jans\n",
    "\n",
    "\n",
    "def imprimir_janelas(jans, ano):\n",
    "    print(f\"\\n=== Janelas definidas para {ano} ===\")\n",
    "    for nome, (ini, fim) in sorted(jans.items()):\n",
    "        dias = (fim - ini).days\n",
    "        print(f\"  {nome:25} → {ini.date()} até {fim.date()} ({dias:3d} dias)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "1ddf8568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Janelas definidas para 2022 ===\n",
      "  antecipacao_45            → 2022-08-01 até 2022-09-30 ( 60 dias)\n",
      "  antecipacao_60            → 2022-07-11 até 2022-09-30 ( 81 dias)\n",
      "  ciclo_1sem                → 2022-01-03 até 2022-06-30 (178 dias)\n",
      "  ciclo_2sem                → 2022-07-01 até 2022-12-30 (182 dias)\n",
      "  entre_turnos              → 2022-09-26 até 2022-10-28 ( 32 dias)\n",
      "  espelho_antecipacao_45    → 2021-08-02 até 2021-09-30 ( 59 dias)\n",
      "  espelho_antecipacao_60    → 2021-07-12 até 2021-09-30 ( 80 dias)\n",
      "  espelho_ciclo_1sem        → 2021-01-04 até 2021-06-30 (177 dias)\n",
      "  espelho_ciclo_2sem        → 2021-07-01 até 2021-12-30 (182 dias)\n",
      "  espelho_entre_turnos      → 2021-09-27 até 2021-10-28 ( 31 dias)\n",
      "  espelho_estendida         → 2021-07-01 até 2021-12-30 (182 dias)\n",
      "  espelho_reacao_ampla_1t   → 2021-09-06 até 2021-11-01 ( 56 dias)\n",
      "  espelho_reacao_curta_1t   → 2021-09-27 até 2021-10-11 ( 14 dias)\n",
      "  espelho_reacao_curta_2t   → 2021-10-25 até 2021-11-08 ( 14 dias)\n",
      "  espelho_reacao_media_1t   → 2021-09-20 até 2021-10-18 ( 28 dias)\n",
      "  espelho_reacao_media_2t   → 2021-10-18 até 2021-11-15 ( 28 dias)\n",
      "  estendida                 → 2022-07-01 até 2022-12-30 (182 dias)\n",
      "  reacao_ampla_1t           → 2022-09-05 até 2022-10-31 ( 56 dias)\n",
      "  reacao_curta_1t           → 2022-09-26 até 2022-10-10 ( 14 dias)\n",
      "  reacao_curta_2t           → 2022-10-24 até 2022-11-07 ( 14 dias)\n",
      "  reacao_media_1t           → 2022-09-19 até 2022-10-17 ( 28 dias)\n",
      "  reacao_media_2t           → 2022-10-17 até 2022-11-14 ( 28 dias)\n"
     ]
    }
   ],
   "source": [
    "bdates = pd.date_range(\"2000-01-01\", \"2023-12-31\", freq='B')\n",
    "jans = coletar_todas_janelas(bdates, 2022)\n",
    "imprimir_janelas(jans, 2022)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50ef67a",
   "metadata": {},
   "source": [
    "## ============================================================================\n",
    "## ETAPA 4 – MODELOS DE ESTIMAÇÃO E INFERÊNCIA ESTATÍSTICA\n",
    "## ============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "bc00f987",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimar_ols_simples(ret_y, ret_x):\n",
    "    \"\"\"OLS simples: R_i = α + β·R_m + ε (Modelo de Mercado)\"\"\"\n",
    "    df = pd.DataFrame({\"y\": ret_y, \"x\": ret_x}).dropna()\n",
    "    if len(df) < 30:\n",
    "        return None\n",
    "    X = np.column_stack([np.ones(len(df)), df[\"x\"].values])\n",
    "    Y = df[\"y\"].values\n",
    "    try:\n",
    "        coef = np.linalg.lstsq(X, Y, rcond=None)[0]\n",
    "        resid = Y - X @ coef\n",
    "        ss_res = (resid**2).sum()\n",
    "        ss_tot = ((Y - Y.mean())**2).sum()\n",
    "        return {\n",
    "            \"alpha\": coef[0],\n",
    "            \"beta\": coef[1],\n",
    "            \"sigma_resid\": np.sqrt(ss_res / (len(df) - 2)),\n",
    "            \"r_squared\": 1 - ss_res / ss_tot if ss_tot > 0 else 0,\n",
    "            \"n_obs_est\": len(df)\n",
    "        }\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "77562f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimar_ols_hac(ret_y, ret_x, maxlags=1):\n",
    "    \"\"\"OLS com erros padrão HAC (Newey-West) — mais robusto\"\"\"\n",
    "    import statsmodels.api as sm\n",
    "    df = pd.DataFrame({\"y\": ret_y, \"x\": ret_x}).dropna()\n",
    "    if len(df) < 30:\n",
    "        return None\n",
    "    X = sm.add_constant(df[\"x\"])\n",
    "    try:\n",
    "        model = sm.OLS(df[\"y\"], X).fit(cov_type=\"HAC\", cov_kwds={\"maxlags\": maxlags})\n",
    "        return {\n",
    "            \"alpha\": model.params.iloc[0],\n",
    "            \"beta\": model.params.iloc[1],\n",
    "            \"alpha_pv\": model.pvalues.iloc[0],\n",
    "            \"beta_pv\": model.pvalues.iloc[1],\n",
    "            \"sigma_resid\": np.sqrt(model.mse_resid),\n",
    "            \"r_squared\": model.rsquared,\n",
    "            \"n_obs_est\": int(model.nobs)\n",
    "        }\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "b6d8c677",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimar_ff3(ret_y, df_factors_window):\n",
    "    \"\"\"Fama-French 3 fatores (NEFIN)\"\"\"\n",
    "    import statsmodels.api as sm\n",
    "    common = ret_y.index.intersection(df_factors_window.index)\n",
    "    if len(common) < 30:\n",
    "        return None\n",
    "    y = ret_y.loc[common]\n",
    "    fac = df_factors_window.loc[common]\n",
    "    y_excess = y - fac.get(\"Rf\", 0)\n",
    "    X_cols = [c for c in [\"Mkt_Rf\", \"SMB\", \"HML\"] if c in fac.columns]\n",
    "    if len(X_cols) < 2:\n",
    "        return None\n",
    "    X = sm.add_constant(fac[X_cols])\n",
    "    try:\n",
    "        model = sm.OLS(y_excess, X).fit(cov_type=\"HAC\", cov_kwds={\"maxlags\": 1})\n",
    "        return {\n",
    "            \"alpha_ff3\": model.params.iloc[0],\n",
    "            \"beta_mkt\": model.params.get(\"Mkt_Rf\", np.nan),\n",
    "            \"beta_smb\": model.params.get(\"SMB\", np.nan),\n",
    "            \"beta_hml\": model.params.get(\"HML\", np.nan),\n",
    "            \"alpha_pv_ff3\": model.pvalues.iloc[0],\n",
    "            \"sigma_resid_ff3\": np.sqrt(model.mse_resid),\n",
    "            \"r_squared_ff3\": model.rsquared,\n",
    "            \"n_obs_est_ff3\": int(model.nobs)\n",
    "        }\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "\n",
    "def calcular_ar(ret_y, ret_x, alpha, beta):\n",
    "    \"\"\"Calcula Retornos Anormais (AR) - Modelo de Mercado\"\"\"\n",
    "    df = pd.DataFrame({\"y\": ret_y, \"x\": ret_x}).dropna()\n",
    "    return df[\"y\"] - (alpha + beta * df[\"x\"])\n",
    "\n",
    "\n",
    "def calcular_ar_ff3(ret_y, df_factors_window, params_ff3):\n",
    "    \"\"\"Calcula Retornos Anormais - Fama-French 3\"\"\"\n",
    "    common = ret_y.index.intersection(df_factors_window.index)\n",
    "    if len(common) == 0:\n",
    "        return pd.Series(dtype=float)\n",
    "    y = ret_y.loc[common]\n",
    "    fac = df_factors_window.loc[common]\n",
    "    rf = fac.get(\"Rf\", 0)\n",
    "    expected = (params_ff3[\"alpha_ff3\"] +\n",
    "                params_ff3.get(\"beta_mkt\", 0) * fac.get(\"Mkt_Rf\", 0) +\n",
    "                params_ff3.get(\"beta_smb\", 0) * fac.get(\"SMB\", 0) +\n",
    "                params_ff3.get(\"beta_hml\", 0) * fac.get(\"HML\", 0))\n",
    "    return (y - rf) - expected\n",
    "\n",
    "\n",
    "def tstat_car_silva(car_values, ar_series_list, n_dias):\n",
    "    \"\"\"Teste t corrigido por autocorrelação (Silva et al., 2015)\"\"\"\n",
    "    n = len(car_values)\n",
    "    if n < 2:\n",
    "        return {\"t_silva\": np.nan, \"p_silva\": np.nan}\n",
    "    car_mean = np.mean(car_values)\n",
    "    variances, covariances = [], []\n",
    "    for ar_s in ar_series_list:\n",
    "        arr = np.array(ar_s.dropna()) if hasattr(ar_s, 'dropna') else np.array(ar_s)\n",
    "        if len(arr) < 2: continue\n",
    "        variances.append(np.var(arr, ddof=1))\n",
    "        if len(arr) > 2:\n",
    "            covariances.append(np.cov(arr[:-1], arr[1:])[0, 1])\n",
    "    if not variances:\n",
    "        return {\"t_silva\": np.nan, \"p_silva\": np.nan}\n",
    "    t = max(n_dias, 1)\n",
    "    csd_sq = t * np.mean(variances) + 2 * max(t-1, 0) * (np.mean(covariances) if covariances else 0)\n",
    "    if csd_sq <= 0:\n",
    "        return {\"t_silva\": np.nan, \"p_silva\": np.nan}\n",
    "    t_stat = car_mean * np.sqrt(n) / np.sqrt(csd_sq)\n",
    "    p_val = 2 * (1 - stats.t.cdf(abs(t_stat), max(n-1, 1)))\n",
    "    return {\"t_silva\": t_stat, \"p_silva\": p_val}\n",
    "\n",
    "\n",
    "def tstat_car_bmp(car_values, sigma_resids, n_dias):\n",
    "    \"\"\"Teste BMP (1991) - controla variância induzida pelo evento\"\"\"\n",
    "    n = len(car_values)\n",
    "    if n < 2:\n",
    "        return {\"t_bmp\": np.nan, \"p_bmp\": np.nan}\n",
    "    denom = sigma_resids * np.sqrt(max(n_dias, 1))\n",
    "    scar = np.where(denom > 0, car_values / denom, np.nan)\n",
    "    scar = scar[~np.isnan(scar)]\n",
    "    if len(scar) < 2:\n",
    "        return {\"t_bmp\": np.nan, \"p_bmp\": np.nan}\n",
    "    s_std = np.std(scar, ddof=1)\n",
    "    if s_std == 0:\n",
    "        return {\"t_bmp\": np.nan, \"p_bmp\": np.nan}\n",
    "    t_bmp = np.mean(scar) * np.sqrt(len(scar)) / s_std\n",
    "    p_bmp = 2 * (1 - stats.t.cdf(abs(t_bmp), len(scar)-1))\n",
    "    return {\"t_bmp\": t_bmp, \"p_bmp\": p_bmp}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ea16ea",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
